{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from captcha.image import ImageCaptcha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "width = 100\n",
    "height = 60\n",
    "charset = '0123456789'\n",
    "captchaLength = 4\n",
    "imageSize = width * height\n",
    "alpha = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCode():\n",
    "    return ''.join(map(\n",
    "        lambda x: charset[x], \n",
    "        np.random.randint(0, len(charset), captchaLength)\n",
    "    ))\n",
    "captcha = ImageCaptcha(width=width, height=height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getData(n = 10):    \n",
    "    codeList = [getCode() for _ in range(n)]\n",
    "    imageList = map(lambda code: captcha.generate_image(code), codeList)\n",
    "    return imageList, codeList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.random_normal(shape, stddev=0.01)                                                                                                     \n",
    "    return tf.Variable(initial)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bias_variable(shape):\n",
    "    initial = tf.random_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W): \n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_pool(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义输入输出\n",
    "x = tf.placeholder(tf.float32, shape=[None, imageSize])\n",
    "y = tf.placeholder(tf.float32, shape=[None, len(charset) * captchaLength])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "x_image = tf.reshape(x, shape=[-1, width, height, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义第一层卷积\n",
    "conv_layer1_weight = weight_variable([5, 5, 1, 32])\n",
    "conv_layer1_bias = bias_variable([32])\n",
    "pool_layer1 = max_pool(\n",
    "    tf.nn.relu(conv2d(x_image, conv_layer1_weight) + conv_layer1_bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义第二层卷积\n",
    "conv_layer2_weight = weight_variable([5, 5, 32, 64])\n",
    "conv_layer2_bias = bias_variable([64])\n",
    "pool_layer2 = max_pool(\n",
    "    tf.nn.relu(conv2d(pool_layer1, conv_layer2_weight) + conv_layer2_bias)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义第三层卷积\n",
    "conv_layer3_weight = weight_variable([5, 5, 64, 64])\n",
    "conv_layer3_bias = bias_variable([64])\n",
    "pool_layer3 = max_pool(\n",
    "    tf.nn.relu(conv2d(pool_layer2, conv_layer3_weight) + conv_layer3_bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastWidth = int(round(width / 8))\n",
    "lastHeight = int(round(height / 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义全连接层\n",
    "fc_layer_weight = weight_variable([lastWidth * lastHeight * 64, 1024])\n",
    "fc_layer_bias = bias_variable([1024])\n",
    "pool_layer3_flat = tf.reshape(pool_layer3, [-1, lastWidth * lastHeight * 64])\n",
    "fc_layer = tf.nn.relu(tf.add(tf.matmul(pool_layer3_flat, fc_layer_weight), fc_layer_bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout层\n",
    "fc_layer_drop = tf.nn.dropout(fc_layer, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readout层\n",
    "output_layer_weight = weight_variable([1024, len(charset) * captchaLength])\n",
    "output_layer_bias = bias_variable([len(charset) * captchaLength])\n",
    "y_conv = tf.add(tf.matmul(fc_layer_drop, output_layer_weight), output_layer_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义输出函数\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=y_conv))\n",
    "optimizer = tf.train.AdamOptimizer(alpha).minimize(loss)\n",
    "prediction = tf.argmax(tf.reshape(y_conv, [-1, captchaLength, len(charset)]), 2)\n",
    "correct = tf.argmax(tf.reshape(y, [-1, captchaLength, len(charset)]), 2)                                                           \n",
    "correct_prediction = tf.equal(prediction, correct)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化session\n",
    "saver = tf.train.Saver()\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imageToVertor(image):\n",
    "    \"\"\" 将图片转化为向量表示 \"\"\"\n",
    "    image = image.convert(\"L\")\n",
    "    image = np.asarray(image)\n",
    "    image = image.reshape([width * height]) / 255\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def codeToVertor(code):\n",
    "    \"\"\" 将验证码转化为向量表示 \"\"\"\n",
    "    labels = np.zeros([captchaLength, len(charset)])\n",
    "    for i in range(captchaLength):\n",
    "        labels[i, charset.index(code[i])] = 1\n",
    "    return labels.reshape(len(charset) * captchaLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]: loss: 0.690429\n",
      "[2]: loss: 0.610761\n",
      "[3]: loss: 0.464669\n",
      "[4]: loss: 0.319703\n",
      "[5]: loss: 0.466319\n",
      "[6]: loss: 0.451865\n",
      "[7]: loss: 0.380793\n",
      "[8]: loss: 0.323238\n",
      "[9]: loss: 0.350861\n",
      "[10]: loss: 0.367005\n",
      "[11]: loss: 0.377611\n",
      "[12]: loss: 0.366303\n",
      "[13]: loss: 0.348796\n",
      "[14]: loss: 0.333631\n",
      "[15]: loss: 0.336407\n",
      "[16]: loss: 0.343376\n",
      "[17]: loss: 0.338347\n",
      "[18]: loss: 0.346365\n",
      "[19]: loss: 0.347115\n",
      "[20]: loss: 0.357813\n",
      "[21]: loss: 0.338703\n",
      "[22]: loss: 0.335340\n",
      "[23]: loss: 0.347702\n",
      "[24]: loss: 0.341254\n",
      "[25]: loss: 0.336141\n",
      "[26]: loss: 0.332695\n",
      "[27]: loss: 0.332490\n",
      "[28]: loss: 0.342500\n",
      "[29]: loss: 0.337657\n",
      "[30]: loss: 0.334766\n",
      "[31]: loss: 0.334872\n",
      "[32]: loss: 0.334144\n",
      "[33]: loss: 0.331498\n",
      "[34]: loss: 0.330046\n",
      "[35]: loss: 0.341723\n",
      "[36]: loss: 0.327263\n",
      "[37]: loss: 0.321013\n",
      "[38]: loss: 0.325559\n",
      "[39]: loss: 0.325214\n",
      "[40]: loss: 0.340154\n",
      "[41]: loss: 0.335449\n",
      "[42]: loss: 0.325814\n",
      "[43]: loss: 0.333847\n",
      "[44]: loss: 0.336624\n",
      "[45]: loss: 0.335985\n",
      "[46]: loss: 0.337576\n",
      "[47]: loss: 0.334604\n",
      "[48]: loss: 0.334794\n",
      "[49]: loss: 0.327181\n",
      "[50]: loss: 0.332522\n",
      "[51]: loss: 0.334286\n",
      "[52]: loss: 0.339997\n",
      "[53]: loss: 0.339713\n",
      "[54]: loss: 0.326451\n",
      "[55]: loss: 0.341128\n",
      "[56]: loss: 0.338216\n",
      "[57]: loss: 0.332701\n",
      "[58]: loss: 0.331531\n",
      "[59]: loss: 0.336514\n",
      "[60]: loss: 0.334422\n",
      "[61]: loss: 0.350201\n",
      "[62]: loss: 0.333864\n",
      "[63]: loss: 0.332143\n",
      "[64]: loss: 0.332798\n",
      "[65]: loss: 0.325677\n",
      "[66]: loss: 0.325472\n",
      "[67]: loss: 0.324866\n",
      "[68]: loss: 0.339655\n",
      "[69]: loss: 0.321979\n",
      "[70]: loss: 0.332751\n",
      "[71]: loss: 0.325496\n",
      "[72]: loss: 0.327980\n",
      "[73]: loss: 0.330759\n",
      "[74]: loss: 0.330803\n",
      "[75]: loss: 0.334195\n",
      "[76]: loss: 0.327015\n",
      "[77]: loss: 0.340059\n",
      "[78]: loss: 0.331941\n",
      "[79]: loss: 0.338206\n",
      "[80]: loss: 0.328851\n",
      "[81]: loss: 0.332600\n",
      "[82]: loss: 0.326970\n",
      "[83]: loss: 0.326734\n",
      "[84]: loss: 0.327633\n",
      "[85]: loss: 0.333841\n",
      "[86]: loss: 0.332946\n",
      "[87]: loss: 0.324273\n",
      "[88]: loss: 0.332244\n",
      "[89]: loss: 0.335558\n",
      "[90]: loss: 0.333794\n",
      "[91]: loss: 0.326458\n",
      "[92]: loss: 0.331525\n",
      "[93]: loss: 0.334645\n",
      "[94]: loss: 0.327757\n",
      "[95]: loss: 0.330410\n",
      "[96]: loss: 0.330315\n",
      "[97]: loss: 0.338745\n",
      "[98]: loss: 0.329056\n",
      "[99]: loss: 0.329103\n",
      "[100]: loss: 0.332224\n",
      "[101]: loss: 0.325982\n",
      "[102]: loss: 0.330873\n",
      "[103]: loss: 0.330558\n",
      "[104]: loss: 0.330109\n",
      "[105]: loss: 0.330215\n",
      "[106]: loss: 0.333057\n",
      "[107]: loss: 0.334033\n",
      "[108]: loss: 0.336975\n",
      "[109]: loss: 0.333286\n",
      "[110]: loss: 0.339336\n",
      "[111]: loss: 0.326521\n",
      "[112]: loss: 0.318753\n",
      "[113]: loss: 0.332715\n",
      "[114]: loss: 0.323092\n",
      "[115]: loss: 0.337906\n",
      "[116]: loss: 0.335077\n",
      "[117]: loss: 0.330342\n",
      "[118]: loss: 0.339120\n",
      "[119]: loss: 0.330177\n",
      "[120]: loss: 0.329237\n",
      "[121]: loss: 0.328620\n",
      "[122]: loss: 0.331041\n",
      "[123]: loss: 0.327773\n",
      "[124]: loss: 0.324100\n",
      "[125]: loss: 0.322498\n",
      "[126]: loss: 0.327186\n",
      "[127]: loss: 0.335704\n",
      "[128]: loss: 0.327577\n",
      "[129]: loss: 0.324169\n",
      "[130]: loss: 0.339444\n",
      "[131]: loss: 0.327131\n",
      "[132]: loss: 0.324830\n",
      "[133]: loss: 0.334175\n",
      "[134]: loss: 0.330529\n",
      "[135]: loss: 0.335077\n",
      "[136]: loss: 0.324828\n",
      "[137]: loss: 0.332249\n",
      "[138]: loss: 0.335535\n",
      "[139]: loss: 0.326974\n",
      "[140]: loss: 0.322543\n",
      "[141]: loss: 0.333449\n",
      "[142]: loss: 0.329500\n",
      "[143]: loss: 0.323749\n",
      "[144]: loss: 0.328403\n",
      "[145]: loss: 0.333237\n",
      "[146]: loss: 0.333086\n",
      "[147]: loss: 0.331590\n",
      "[148]: loss: 0.329636\n",
      "[149]: loss: 0.321211\n",
      "[150]: loss: 0.331533\n",
      "[151]: loss: 0.326398\n",
      "[152]: loss: 0.328158\n",
      "[153]: loss: 0.330150\n",
      "[154]: loss: 0.325244\n",
      "[155]: loss: 0.320464\n",
      "[156]: loss: 0.337644\n",
      "[157]: loss: 0.336454\n",
      "[158]: loss: 0.326856\n",
      "[159]: loss: 0.326101\n",
      "[160]: loss: 0.334255\n",
      "[161]: loss: 0.327737\n",
      "[162]: loss: 0.334916\n",
      "[163]: loss: 0.335398\n",
      "[164]: loss: 0.332088\n",
      "[165]: loss: 0.330361\n",
      "[166]: loss: 0.325157\n",
      "[167]: loss: 0.331717\n",
      "[168]: loss: 0.327547\n",
      "[169]: loss: 0.326300\n",
      "[170]: loss: 0.324533\n",
      "[171]: loss: 0.322154\n",
      "[172]: loss: 0.336163\n",
      "[173]: loss: 0.328312\n",
      "[174]: loss: 0.338498\n",
      "[175]: loss: 0.340433\n",
      "[176]: loss: 0.331947\n",
      "[177]: loss: 0.328321\n",
      "[178]: loss: 0.330021\n",
      "[179]: loss: 0.332110\n",
      "[180]: loss: 0.325718\n",
      "[181]: loss: 0.340492\n",
      "[182]: loss: 0.328914\n",
      "[183]: loss: 0.326100\n",
      "[184]: loss: 0.326630\n",
      "[185]: loss: 0.322460\n",
      "[186]: loss: 0.321073\n",
      "[187]: loss: 0.323991\n",
      "[188]: loss: 0.334410\n",
      "[189]: loss: 0.333268\n",
      "[190]: loss: 0.329685\n",
      "[191]: loss: 0.325078\n",
      "[192]: loss: 0.336165\n",
      "[193]: loss: 0.329270\n",
      "[194]: loss: 0.330855\n",
      "[195]: loss: 0.337544\n",
      "[196]: loss: 0.323081\n",
      "[197]: loss: 0.327784\n",
      "[198]: loss: 0.330242\n",
      "[199]: loss: 0.324348\n",
      "[200]: loss: 0.328394\n",
      "[201]: loss: 0.327525\n",
      "[202]: loss: 0.330777\n",
      "[203]: loss: 0.331791\n",
      "[204]: loss: 0.332852\n",
      "[205]: loss: 0.332124\n",
      "[206]: loss: 0.330849\n",
      "[207]: loss: 0.322378\n",
      "[208]: loss: 0.325814\n",
      "[209]: loss: 0.332717\n",
      "[210]: loss: 0.325916\n",
      "[211]: loss: 0.328207\n",
      "[212]: loss: 0.331025\n",
      "[213]: loss: 0.327574\n",
      "[214]: loss: 0.329944\n",
      "[215]: loss: 0.320287\n",
      "[216]: loss: 0.334499\n",
      "[217]: loss: 0.328512\n",
      "[218]: loss: 0.327357\n",
      "[219]: loss: 0.323096\n",
      "[220]: loss: 0.333793\n",
      "[221]: loss: 0.328122\n",
      "[222]: loss: 0.323890\n",
      "[223]: loss: 0.328076\n",
      "[224]: loss: 0.323620\n",
      "[225]: loss: 0.331361\n",
      "[226]: loss: 0.328865\n",
      "[227]: loss: 0.332454\n",
      "[228]: loss: 0.323206\n",
      "[229]: loss: 0.324218\n",
      "[230]: loss: 0.318318\n",
      "[231]: loss: 0.332431\n",
      "[232]: loss: 0.335456\n",
      "[233]: loss: 0.325071\n",
      "[234]: loss: 0.334425\n",
      "[235]: loss: 0.331993\n",
      "[236]: loss: 0.327630\n",
      "[237]: loss: 0.337354\n",
      "[238]: loss: 0.330970\n",
      "[239]: loss: 0.330315\n",
      "[240]: loss: 0.324412\n",
      "[241]: loss: 0.327426\n",
      "[242]: loss: 0.323456\n",
      "[243]: loss: 0.322362\n",
      "[244]: loss: 0.330272\n",
      "[245]: loss: 0.326366\n",
      "[246]: loss: 0.334717\n",
      "[247]: loss: 0.337582\n",
      "[248]: loss: 0.336821\n",
      "[249]: loss: 0.326236\n",
      "[250]: loss: 0.330378\n",
      "[251]: loss: 0.329473\n",
      "[252]: loss: 0.326359\n",
      "[253]: loss: 0.331261\n",
      "[254]: loss: 0.331965\n",
      "[255]: loss: 0.330385\n",
      "[256]: loss: 0.332370\n",
      "[257]: loss: 0.330542\n",
      "[258]: loss: 0.329513\n",
      "[259]: loss: 0.329962\n",
      "[260]: loss: 0.318717\n",
      "[261]: loss: 0.324085\n",
      "[262]: loss: 0.325910\n",
      "[263]: loss: 0.328100\n",
      "[264]: loss: 0.323240\n",
      "[265]: loss: 0.323256\n",
      "[266]: loss: 0.325855\n",
      "[267]: loss: 0.326192\n",
      "[268]: loss: 0.330627\n",
      "[269]: loss: 0.337778\n",
      "[270]: loss: 0.329672\n",
      "[271]: loss: 0.332268\n",
      "[272]: loss: 0.326467\n",
      "[273]: loss: 0.319707\n",
      "[274]: loss: 0.326749\n",
      "[275]: loss: 0.324508\n",
      "[276]: loss: 0.321188\n",
      "[277]: loss: 0.334557\n",
      "[278]: loss: 0.332565\n",
      "[279]: loss: 0.326412\n",
      "[280]: loss: 0.325307\n",
      "[281]: loss: 0.322300\n",
      "[282]: loss: 0.310161\n",
      "[283]: loss: 0.331331\n",
      "[284]: loss: 0.319109\n",
      "[285]: loss: 0.323805\n",
      "[286]: loss: 0.325629\n",
      "[287]: loss: 0.328507\n",
      "[288]: loss: 0.329596\n",
      "[289]: loss: 0.331801\n",
      "[290]: loss: 0.333392\n",
      "[291]: loss: 0.329762\n",
      "[292]: loss: 0.333853\n",
      "[293]: loss: 0.332123\n",
      "[294]: loss: 0.338989\n",
      "[295]: loss: 0.325646\n",
      "[296]: loss: 0.331335\n",
      "[297]: loss: 0.337386\n",
      "[298]: loss: 0.331741\n",
      "[299]: loss: 0.328229\n",
      "[300]: loss: 0.332612\n",
      "[301]: loss: 0.325197\n",
      "[302]: loss: 0.330505\n",
      "[303]: loss: 0.326826\n",
      "[304]: loss: 0.325840\n",
      "[305]: loss: 0.333031\n",
      "[306]: loss: 0.328719\n",
      "[307]: loss: 0.322613\n",
      "[308]: loss: 0.330229\n",
      "[309]: loss: 0.336853\n",
      "[310]: loss: 0.327287\n",
      "[311]: loss: 0.329762\n",
      "[312]: loss: 0.331140\n",
      "[313]: loss: 0.326764\n",
      "[314]: loss: 0.331636\n",
      "[315]: loss: 0.330537\n",
      "[316]: loss: 0.328004\n",
      "[317]: loss: 0.335661\n",
      "[318]: loss: 0.332187\n",
      "[319]: loss: 0.335605\n",
      "[320]: loss: 0.328727\n",
      "[321]: loss: 0.321826\n",
      "[322]: loss: 0.321575\n",
      "[323]: loss: 0.334555\n",
      "[324]: loss: 0.328192\n",
      "[325]: loss: 0.325924\n",
      "[326]: loss: 0.332213\n",
      "[327]: loss: 0.337011\n",
      "[328]: loss: 0.328309\n",
      "[329]: loss: 0.332607\n",
      "[330]: loss: 0.334772\n",
      "[331]: loss: 0.324981\n",
      "[332]: loss: 0.331857\n",
      "[333]: loss: 0.330278\n",
      "[334]: loss: 0.325388\n",
      "[335]: loss: 0.328884\n",
      "[336]: loss: 0.330192\n",
      "[337]: loss: 0.334228\n",
      "[338]: loss: 0.326937\n",
      "[339]: loss: 0.324608\n",
      "[340]: loss: 0.332257\n",
      "[341]: loss: 0.326134\n",
      "[342]: loss: 0.329111\n",
      "[343]: loss: 0.325737\n",
      "[344]: loss: 0.328565\n",
      "[345]: loss: 0.322018\n",
      "[346]: loss: 0.332074\n",
      "[347]: loss: 0.323363\n",
      "[348]: loss: 0.323607\n",
      "[349]: loss: 0.323615\n",
      "[350]: loss: 0.329843\n",
      "[351]: loss: 0.331687\n",
      "[352]: loss: 0.322857\n",
      "[353]: loss: 0.328652\n",
      "[354]: loss: 0.325229\n",
      "[355]: loss: 0.330209\n",
      "[356]: loss: 0.319990\n",
      "[357]: loss: 0.325655\n",
      "[358]: loss: 0.340662\n",
      "[359]: loss: 0.329849\n",
      "[360]: loss: 0.328715\n",
      "[361]: loss: 0.337232\n",
      "[362]: loss: 0.327327\n",
      "[363]: loss: 0.329144\n",
      "[364]: loss: 0.327700\n",
      "[365]: loss: 0.328316\n",
      "[366]: loss: 0.323688\n",
      "[367]: loss: 0.325818\n",
      "[368]: loss: 0.329531\n",
      "[369]: loss: 0.327515\n",
      "[370]: loss: 0.325614\n",
      "[371]: loss: 0.325436\n",
      "[372]: loss: 0.327893\n",
      "[373]: loss: 0.325809\n",
      "[374]: loss: 0.325687\n",
      "[375]: loss: 0.326644\n",
      "[376]: loss: 0.327687\n",
      "[377]: loss: 0.330085\n",
      "[378]: loss: 0.332893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[379]: loss: 0.323300\n",
      "[380]: loss: 0.331969\n",
      "[381]: loss: 0.325438\n",
      "[382]: loss: 0.326304\n",
      "[383]: loss: 0.331097\n",
      "[384]: loss: 0.326804\n",
      "[385]: loss: 0.326017\n",
      "[386]: loss: 0.327932\n",
      "[387]: loss: 0.325412\n",
      "[388]: loss: 0.322468\n",
      "[389]: loss: 0.328122\n",
      "[390]: loss: 0.333956\n",
      "[391]: loss: 0.331000\n",
      "[392]: loss: 0.328226\n",
      "[393]: loss: 0.326085\n",
      "[394]: loss: 0.329281\n",
      "[395]: loss: 0.326034\n",
      "[396]: loss: 0.330590\n",
      "[397]: loss: 0.325503\n",
      "[398]: loss: 0.324399\n",
      "[399]: loss: 0.327867\n",
      "[400]: loss: 0.325860\n",
      "[401]: loss: 0.332431\n",
      "[402]: loss: 0.337104\n",
      "[403]: loss: 0.328161\n",
      "[404]: loss: 0.327221\n",
      "[405]: loss: 0.327249\n",
      "[406]: loss: 0.322370\n",
      "[407]: loss: 0.324754\n",
      "[408]: loss: 0.335523\n",
      "[409]: loss: 0.330091\n",
      "[410]: loss: 0.329635\n",
      "[411]: loss: 0.329600\n",
      "[412]: loss: 0.326460\n",
      "[413]: loss: 0.331663\n",
      "[414]: loss: 0.327535\n",
      "[415]: loss: 0.328605\n",
      "[416]: loss: 0.324455\n",
      "[417]: loss: 0.322541\n",
      "[418]: loss: 0.334118\n",
      "[419]: loss: 0.327895\n",
      "[420]: loss: 0.325881\n",
      "[421]: loss: 0.328798\n",
      "[422]: loss: 0.328621\n",
      "[423]: loss: 0.326543\n",
      "[424]: loss: 0.335391\n",
      "[425]: loss: 0.334053\n",
      "[426]: loss: 0.329467\n",
      "[427]: loss: 0.326307\n",
      "[428]: loss: 0.336685\n",
      "[429]: loss: 0.328062\n",
      "[430]: loss: 0.329640\n",
      "[431]: loss: 0.331293\n",
      "[432]: loss: 0.327518\n",
      "[433]: loss: 0.335326\n",
      "[434]: loss: 0.328619\n",
      "[435]: loss: 0.322879\n",
      "[436]: loss: 0.327070\n",
      "[437]: loss: 0.332874\n",
      "[438]: loss: 0.325414\n",
      "[439]: loss: 0.330173\n",
      "[440]: loss: 0.323574\n",
      "[441]: loss: 0.322822\n",
      "[442]: loss: 0.327442\n",
      "[443]: loss: 0.329365\n",
      "[444]: loss: 0.330993\n",
      "[445]: loss: 0.329314\n",
      "[446]: loss: 0.333226\n",
      "[447]: loss: 0.326440\n",
      "[448]: loss: 0.330066\n",
      "[449]: loss: 0.325154\n",
      "[450]: loss: 0.330515\n",
      "[451]: loss: 0.323102\n",
      "[452]: loss: 0.326891\n",
      "[453]: loss: 0.328195\n",
      "[454]: loss: 0.325754\n",
      "[455]: loss: 0.327382\n",
      "[456]: loss: 0.328950\n",
      "[457]: loss: 0.324332\n",
      "[458]: loss: 0.330068\n",
      "[459]: loss: 0.335224\n",
      "[460]: loss: 0.326683\n",
      "[461]: loss: 0.327667\n",
      "[462]: loss: 0.325991\n",
      "[463]: loss: 0.323227\n",
      "[464]: loss: 0.326757\n",
      "[465]: loss: 0.327133\n",
      "[466]: loss: 0.326355\n",
      "[467]: loss: 0.326757\n",
      "[468]: loss: 0.325512\n",
      "[469]: loss: 0.322362\n",
      "[470]: loss: 0.329034\n",
      "[471]: loss: 0.323794\n",
      "[472]: loss: 0.333713\n",
      "[473]: loss: 0.333087\n",
      "[474]: loss: 0.338112\n",
      "[475]: loss: 0.323724\n",
      "[476]: loss: 0.324508\n",
      "[477]: loss: 0.324854\n",
      "[478]: loss: 0.327085\n",
      "[479]: loss: 0.334999\n",
      "[480]: loss: 0.327912\n",
      "[481]: loss: 0.331961\n",
      "[482]: loss: 0.325280\n",
      "[483]: loss: 0.326397\n",
      "[484]: loss: 0.333358\n",
      "[485]: loss: 0.329786\n",
      "[486]: loss: 0.326211\n",
      "[487]: loss: 0.325621\n",
      "[488]: loss: 0.328463\n",
      "[489]: loss: 0.325717\n",
      "[490]: loss: 0.326558\n",
      "[491]: loss: 0.332149\n",
      "[492]: loss: 0.325964\n",
      "[493]: loss: 0.324648\n",
      "[494]: loss: 0.325940\n",
      "[495]: loss: 0.328152\n",
      "[496]: loss: 0.329898\n",
      "[497]: loss: 0.329539\n",
      "[498]: loss: 0.331565\n",
      "[499]: loss: 0.325028\n",
      "[500]: loss: 0.326021\n",
      "[501]: loss: 0.331328\n",
      "[502]: loss: 0.331044\n",
      "[503]: loss: 0.337982\n",
      "[504]: loss: 0.328938\n",
      "[505]: loss: 0.325466\n",
      "[506]: loss: 0.328130\n",
      "[507]: loss: 0.325311\n",
      "[508]: loss: 0.335104\n",
      "[509]: loss: 0.324275\n",
      "[510]: loss: 0.335604\n",
      "[511]: loss: 0.330751\n",
      "[512]: loss: 0.324378\n",
      "[513]: loss: 0.323828\n",
      "[514]: loss: 0.336102\n",
      "[515]: loss: 0.333063\n",
      "[516]: loss: 0.323097\n",
      "[517]: loss: 0.330772\n",
      "[518]: loss: 0.328211\n",
      "[519]: loss: 0.326171\n",
      "[520]: loss: 0.328533\n",
      "[521]: loss: 0.324336\n",
      "[522]: loss: 0.327321\n",
      "[523]: loss: 0.326349\n",
      "[524]: loss: 0.329708\n",
      "[525]: loss: 0.327937\n",
      "[526]: loss: 0.322362\n",
      "[527]: loss: 0.322266\n",
      "[528]: loss: 0.330032\n",
      "[529]: loss: 0.329911\n",
      "[530]: loss: 0.324945\n",
      "[531]: loss: 0.326991\n",
      "[532]: loss: 0.330849\n",
      "[533]: loss: 0.324736\n",
      "[534]: loss: 0.323819\n",
      "[535]: loss: 0.323297\n",
      "[536]: loss: 0.325896\n",
      "[537]: loss: 0.334844\n",
      "[538]: loss: 0.326809\n",
      "[539]: loss: 0.325178\n",
      "[540]: loss: 0.330376\n",
      "[541]: loss: 0.328394\n",
      "[542]: loss: 0.325981\n",
      "[543]: loss: 0.334150\n",
      "[544]: loss: 0.328465\n",
      "[545]: loss: 0.319379\n",
      "[546]: loss: 0.322173\n",
      "[547]: loss: 0.326328\n",
      "[548]: loss: 0.329558\n",
      "[549]: loss: 0.334555\n",
      "[550]: loss: 0.329489\n",
      "[551]: loss: 0.318722\n",
      "[552]: loss: 0.328988\n",
      "[553]: loss: 0.336925\n",
      "[554]: loss: 0.328950\n",
      "[555]: loss: 0.329553\n",
      "[556]: loss: 0.322617\n",
      "[557]: loss: 0.330260\n",
      "[558]: loss: 0.334144\n",
      "[559]: loss: 0.334211\n",
      "[560]: loss: 0.331803\n",
      "[561]: loss: 0.327230\n",
      "[562]: loss: 0.326677\n",
      "[563]: loss: 0.325582\n",
      "[564]: loss: 0.329900\n",
      "[565]: loss: 0.328329\n",
      "[566]: loss: 0.329331\n",
      "[567]: loss: 0.334179\n",
      "[568]: loss: 0.329017\n",
      "[569]: loss: 0.327007\n",
      "[570]: loss: 0.327466\n",
      "[571]: loss: 0.324774\n",
      "[572]: loss: 0.328118\n",
      "[573]: loss: 0.320409\n",
      "[574]: loss: 0.330273\n",
      "[575]: loss: 0.329896\n",
      "[576]: loss: 0.330695\n",
      "[577]: loss: 0.330680\n",
      "[578]: loss: 0.327198\n",
      "[579]: loss: 0.326330\n",
      "[580]: loss: 0.333288\n",
      "[581]: loss: 0.324714\n",
      "[582]: loss: 0.327472\n",
      "[583]: loss: 0.329317\n",
      "[584]: loss: 0.325908\n",
      "[585]: loss: 0.327323\n",
      "[586]: loss: 0.327588\n",
      "[587]: loss: 0.323463\n",
      "[588]: loss: 0.330867\n",
      "[589]: loss: 0.326775\n",
      "[590]: loss: 0.326732\n",
      "[591]: loss: 0.326058\n",
      "[592]: loss: 0.325722\n",
      "[593]: loss: 0.329635\n",
      "[594]: loss: 0.330360\n",
      "[595]: loss: 0.328933\n",
      "[596]: loss: 0.325527\n",
      "[597]: loss: 0.330850\n",
      "[598]: loss: 0.325784\n",
      "[599]: loss: 0.329396\n"
     ]
    }
   ],
   "source": [
    "for step in range(1, 1000):\n",
    "    imageList, codeList = getData(10)\n",
    "    x_data = map(imageToVertor, imageList)\n",
    "    y_data = map(codeToVertor, codeList)\n",
    "    _, l = session.run([optimizer, loss], feed_dict={x: x_data, y: y_data, keep_prob: .75})\n",
    "    print '[%d]: loss: %f' % (step, l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
